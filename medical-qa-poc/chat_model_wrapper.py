# chat_model_wrapper.py

import os
import google.generativeai as genai
from dotenv import load_dotenv

load_dotenv(override=True)

class ChatRefiner:
    def __init__(self, model_name="gemini-1.5-flash"):
        """
        Initializes the Gemini Pro chat model.
        """
        self.api_key = os.getenv("GOOGLE_API_KEY")
        if not self.api_key:
            raise ValueError("GOOGLE_API_KEY not set in environment or .env file.")
        
        genai.configure(api_key=self.api_key)
        self.model = genai.GenerativeModel(model_name)
    
    def reframe(self, vague_question: str) -> str:
        """
        Uses Gemini to reframe a vague or unsupported question into a more precise medical query.
        
        Args:
            vague_question (str): The original unclear or broad question.
        
        Returns:
            A clearer, medically-oriented version of the question.
        """
        prompt = f"""
        You are a medical assistant. Rephrase the following user question into a clearer, more medically relevant format.
        Avoid hallucinations—just make it clearer and more specific if possible.

        Original Question: {vague_question}

        Rephrased Medical Question:
        """
        try:
            response = self.model.generate_content(prompt)
            return response.text.strip()
        except Exception as e:
            print(f"[GeminiReframer] Error: {e}")
            return vague_question  # fallback to original


    def refine(self, context: str, answer: str) -> str:
        """
        Uses Gemini to improve and clarify the answer text.

        Args:
            context: The user’s question or prompt.
            answer: The original system-generated answer.

        Returns:
            A refined answer as a string.
        """
        prompt = f"""
        You are a helpful assistant. Given the user's question and the system answer,
        rewrite the answer to be more natural, friendly, and informative without changing the meaning.

        Question: {context}
        Answer: {answer}

        Improved Answer:
        """

        try:
            response = self.model.generate_content(prompt)
            return response.text.strip()
        except Exception as e:
            print(f"[GeminiRefiner] Error: {e}")
            return answer  # fallback to raw answer
        
    def answer(self, question: str) -> str:
        """
        Directly answers a user question using the Gemini model.

        Args:
            question (str): The user's question.

        Returns:
            str: Answer generated by Gemini.
        """
        prompt = f"""
        You are a knowledgeable and accurate medical assistant. Answer the user's question factually and clearly.

        User Question: {question}
        """
        try:
            response = self.model.generate_content(prompt)
            return response.text.strip()
        except Exception as e:
            print(f"[GeminiAnswer] Error: {e}")
            return "I'm sorry, I couldn't generate a response right now."
